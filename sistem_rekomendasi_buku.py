# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dj2VYs4NYI3v4pps16DHwF5UpAUWMAJO

## Import Library
"""

import pandas as pd
import numpy as np
import os
import zipfile

from google.colab import files

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

!pip install -q kaggle

"""## Load Data"""

files.upload()

!mkdir book_recommendation
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d arashnic/book-recommendation-dataset -p /content/book_recommendation/

!unzip '/content/book_recommendation/book-recommendation-dataset.zip' -d /content/book_recommendation/

books = pd.read_csv('/content/book_recommendation/Books.csv', low_memory=False)
users = pd.read_csv('/content/book_recommendation/Users.csv', low_memory=False)
ratings = pd.read_csv('/content/book_recommendation/Ratings.csv', low_memory=False)

"""## Data Understanding"""

books.head()

users.head()

ratings.head()

"""### Insight

*   Dataset terdiri dari informasi buku pengguna dan rating.
*   Dataset `Ratings` merupakan yang terbesar, hal ini mencerminkan banyaknya interaksi pengguna dengan buku.

## Univariate Exploratory Data Analysis

Variabel pada dataset Book Recommendation yaitu sebagai berikut :

*   Books   :  menyediakan informasi detail tentang buku.
*   Users   :  memberikan informasi tentang pengguna yang memberikan rating.
*   Ratings :  berisi interaksi antara pengguna dan buku dalam bentuk rating. Dat ini menunjukkan preferensi pengguna terhadap buku-buku tertentu. Dataset ini merupakan yang terbesar, hal ini menunjukkan bahwa ada banyak interaksi rating yang tercatat.

Pada model rekomendasi variabel `books` dan `ratings` yang akan digunakan. Untuk variabel `users` untuk melihat profil pengguna.

### Books Variabel

**Informasi Dataset**
"""

print("\nInformasi Dataset :")
books.info()

"""*   File books.csv memiliki 271360 entries.   
*   Kolom `Book-Title`, `Book-Author`, `Year-Of-Publication`, dan `Publisher` memiliki tipe data *object* yang sesuai untuk menyimpan teks dan nama.
*   Kolom `ISBN` juga bertipe *object*, yang merupakan identifikasi unik untuk buku.
*   Kolom `Image-URL-S, Image-URL-M, dan Image-URL-L` juga bertipe *object*, menyimpan tautan gambar sampul buku dalam berbagai ukuran.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Mengubah tipe data 'Year-Of-Publication' menjadi numerik, menangani error
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')

# Menghapus data dengan tahun publikasi yang tidak valid (misalnya, tahun > tahun saat ini)
current_year = pd.to_datetime('today').year
books_valid_year = books[(books['Year-Of-Publication'] > 0) & (books['Year-Of-Publication'] <= current_year)]

# Membuat plot distribusi tahun publikasi
plt.figure(figsize=(12, 6))
sns.histplot(books_valid_year['Year-Of-Publication'], bins=30, kde=True)
plt.title('Distribusi Tahun Publikasi Buku')
plt.xlabel('Tahun Publikasi')
plt.ylabel('Jumlah Buku')
plt.show()

"""Dapat dilihat bahwa rentang tahun publikasi buku paling banyak di tahun 2000an.

**Mengurangi entitas pada books variabel**
"""

books = books[1:7001]
books.info()

"""Dikarenakan terdapat lebih dari 200 ribu data, maka akan diambil 7000 data pertama dari `books` untuk digunakan dalam pembuatan model sistem rekomendasi.

**Melihat judul buku**
"""

print("Banyak jumlah data :", len(books['Book-Title'].unique()))
print("Book Title         :", books['Book-Title'].unique())

"""*   Terdapat 6750 judul buku yang unik dalam dataset `books`.
*   Menampilkan beberapa judul buku unik. Menunjukkan keragaman judul buku yang ada dalam dataset. Hal ini mengindikasikan bahwa dataset ini mencakup berbagai macam buku.

### Users Variabel

**Informasi Dataset**
"""

print("\nInformasi Dataset :")
users.info()

"""*   File users.csv memiliki 278858 entries.
*   Terdapat kolom `User-ID`, `Location`, dan `Age` pada dataset `users`
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

users['Age'] = pd.to_numeric(users['Age'], errors='coerce')

users_valid_age = users.dropna(subset=['Age'])

users_valid_age = users_valid_age[(users_valid_age['Age'] > 5) & (users_valid_age['Age'] < 100)]

# Membuat plot distribusi usia pengguna
plt.figure(figsize=(12, 6))
sns.histplot(users_valid_age['Age'], bins=30, kde=True)
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Usia')
plt.ylabel('Jumlah Pengguna')
plt.show()

"""Banyak pengguna dalam rentang usia 20-an atau 30-an.

**Mengurangi entitas pada variabel users**
"""

users = users[1:7001]
users.info()

"""Dikarenakan terdapat lebih dari 200 ribu data, maka akan diambil 7000 data pertama dari `users` untuk digunakan dalam pembuatan model sistem rekomendasi.

### Ratings Variabel

**Informasi Dataset**
"""

print("\nInformasi Dataset :")
ratings.info()

"""
*   Kolom `User-ID` , `ISBN`, dan `Book-Rating` semuanya bertipe data integer (*int64*).
*   `User-ID` mengidentifikasi pengguna yang memberikan rating.
*   `ISBN` mengidentifikasi buku yang diberi rating.
*   `Book-Rating` menunjukkan rating yang diberikan oleh pengguna untuk buku-buku tertentu.
*   Tipe data integer untuk ketiga kolom sangat sesuai. `User-ID` dan `ISBN` merupakan ID unik, dan `Book-Rating` merupakan nilai numerik untuk rating.
*   Dataset `Ratings` berisi interaksi antara pengguna dan buku dalam bentuk rating. Dat ini menunjukkan preferensi pengguna terhadap buku-buku tertentu. Dataset ini merupakan yang terbesar, hal ini menunjukkan bahwa ada banyak interaksi rating yang tercatat."""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

ratings['Book-Rating'] = pd.to_numeric(ratings['Book-Rating'], errors='coerce')

ratings_clean = ratings.dropna(subset=['Book-Rating'])

# Membuat plot distribusi rating
plt.figure(figsize=(10, 6))
sns.countplot(x='Book-Rating', data=ratings, palette='viridis', hue='Book-Rating', legend=False)
plt.xlabel('Rating Buku')
plt.ylabel('Jumlah Rating')
plt.show()

"""Diagram diatas menunjukkan rating 0 atau rating rendah lebih condong, hal ini menunjukkan baik sejumlah besar buku yang tidak diberi rating, kecenderungan kuat pengguna untuk tidak menyukai buku atau ada kesalahan dalam mengimput data.

**Mengurangi entitas pada variabel ratings**
"""

ratings = ratings[1:7001]
ratings.info()

"""Dikarenakan terdapat lebih dari 11 juta data, maka akan diambil 7000 data pertama dari `users` untuk digunakan dalam pembuatan model sistem rekomendasi."""

ratings.describe()

"""Untuk skala rating yaitu 0 - 10. Nilai minimal rating yaitu 0 dan nilai maksimumnya yaitu 10.

**Melihat jumlah pengguna memberikan rating, jumlah ISBN, serta jumlah data rating**
"""

print("\nJumlah User-ID     :", len(ratings['User-ID'].unique()))
print("\nJumlah ISBN        :", len(ratings['ISBN'].unique()))
print("\nJumlah Data Rating :", len(ratings))

"""Dapat dilihat bahwa jumlah data pengguna yang memberikan rating sebanyak 678, jumlah buku sebanyak 6600 dan jumlah rating sebanyak 7000.

## Data Preprocessing

**Menghapus fitur yang tidak digunakan**
"""

books = books.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1)

books.head()

"""**Mengecek missing value pada ratings**"""

print("\nJumlah missing value ratings : ")
ratings.isnull().sum()

"""Tidak terdapat missing value pada dataset `ratings`.

**Menghitung jumlah rating dan menggabungkan berdasarkan ISBN**
"""

ratings.groupby('ISBN').sum()

"""**Menggabungkan Ratings dengan Books**"""

ratings_all = ratings
ratings_all.head()

"""Mendefinisikan variabel `ratings_all` dengan variabel `ratings` yang sudah ada sebelumnya."""

books_all = pd.merge(ratings_all, books[['ISBN','Book-Title', 'Book-Author']], on='ISBN', how='left')
books_all.head()

"""*   Menggabungkan dataframe `ratings_all` dengan `books` dan memasukkan ke variabel `books_all`.
*   Karena dilakukan penggabungan fitur maka terdapat nilai yang hilang. Sehingga harus dilakukan pengecekan dan penangan missing value.

## Data Preparation

**Mengatasi missing value**
"""

print("\nJumlah missing value books_all :")
books_all.isnull().sum()

"""Terdapat missing values pada `Book-Title` dan `Book-Author` sebanyak 5971. Dan perlu dilakukan pembersihan data pada missing value."""

print("\nJumlah Keseluruhan Buku :")
len(books_all)

books_all_clean = books_all.dropna()
books_all_clean.isnull().sum()

"""Membersihkan missing value menggunakan fungsi `dropna()`. Dan kini sudah tidak ada missing value.

**Mengurutkan books berdasarkan nomor ISBN**
"""

preparation = books_all_clean.sort_values('ISBN', ascending=True)
preparation.head()

"""**Mengatasi Data Duplikat**"""

preparation = preparation.drop_duplicates('ISBN')
preparation.head()

"""**Konversi data series menjadi list**"""

book_id = preparation['ISBN'].tolist()

book_title = preparation['Book-Title'].tolist()

book_author = preparation['Book-Author'].tolist()

print(len(book_id))
print(len(book_title))
print(len(book_author))

"""**Membuat dictionary baru untuk data Book ID, Book Title, Book Author**"""

new_book = pd.DataFrame({
    'id' : book_id,
    'book_title' : book_title,
    'book_author' : book_author
})

new_book.head()

"""### Insight Data Preprocessing dan Preparation

*   Dataset yang besar (lebih dari 200 ribu entries untuk buku dan pengguna, serta jutaan rating) telah dikurangi menjadi 7000 data pertama untuk setiap dataset untuk kemudahan pemrosesan.
*   Fitur gambar seperti Image-URL-S, Image-URL-M, dan Img-URL-L yang tidak digunakan dalam model rekomendasi telah dihapus.
*   Missing values pada kolom Book-Title dan Book-Author setelah penggabungan dataset `Ratings` dan `Books` berhasil ditangani dengan menghapus baris yang mengandung missing values.
*   Data duplikat berdasarkan ISBN telah dihapus.
*   Data penting seperti ISBN, Book-Title dan Book-Author dikonversi menjadi list dan kemudian dibentuk menjadi DataFramw baru untuk persiapan model.

## Model Development dengan Content Based Filtering

**TF-IDF Vectorizer**

TF-IDF Vectorizer digunakan pada sistem rekomendasi untuk menemukan fitur yang penting dari setiap kategori buku.
"""

tf = TfidfVectorizer()

tf.fit(new_book['book_title'])

tf.get_feature_names_out()

"""**Melakukan fit lalu ditransformasikan ke bentuk metrix**"""

tfidf_matrix = tf.fit_transform(new_book['book_title'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""**Mengubah vektor tf-idf dalam bentuk metrix dengan fungsi todense**"""

tfidf_matrix.todense()

"""**Membuat dataframe untuk melihat tf-idf matrix**"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=new_book.book_author
).sample(1541, axis=1).sample(10, axis=0)

"""**Cosine Similarity**"""

cosine_similar = cosine_similarity(tfidf_matrix)
cosine_similar

"""**Membuat dataframe dari variabel `cosine_similar` dengan baris dan kolom berupa book author**"""

df_cosine_similar = pd.DataFrame(
    cosine_similar,
    index=new_book['book_author'],
    columns=new_book['book_author']
)
print('Shape:', df_cosine_similar.shape)

df_cosine_similar.sample(7, axis=1).sample(10, axis=0)

"""**Mendapatkan Rekomendasi**"""

def book_recommendations(
    author_name,
    data_similarity=df_cosine_similar,
    items=new_book[['book_author', 'book_title']], n=10):

  index = data_similarity.loc[:, author_name].to_numpy().argpartition(
      range(-1, -n, -1)
  )

  closest = data_similarity.columns[index[-1:-(n+2):-1]]

  closest = closest.drop(author_name, errors='ignore')

  return pd.DataFrame(closest).merge(items).head(n)

new_book[new_book.book_author.eq('Peter Carey')]

book_recommendations('Peter Carey')

from sklearn.model_selection import train_test_split # Import untuk membagi data

# Gunakan DataFrame preparation yang sudah bersih dan unik berdasarkan ISBN
preparation = books_all_clean.sort_values('ISBN', ascending=True)
preparation = preparation.drop_duplicates('ISBN')

# Gunakan books_all_clean karena berisi User-ID, ISBN, dan rating
interaction_data = books_all_clean[['User-ID', 'ISBN', 'Book-Rating', 'Book-Title', 'Book-Author']]

train_interaction, test_interaction = train_test_split(
    interaction_data,
    test_size=0.2, # 20% untuk pengujian
    random_state=42
)

print(f"Jumlah data interaksi pelatihan: {len(train_interaction)}")
print(f"Jumlah data interaksi pengujian: {len(test_interaction)}")

# --- Bangun Model Content-Based Filtering dari Data Pelatihan ---

# Ambil buku-buku unik dari data interaksi pelatihan untuk membangun model
train_books = train_interaction.drop_duplicates('ISBN')[['ISBN', 'Book-Title', 'Book-Author']]
train_books = train_books.reset_index(drop=True) # Reset index

print(f"\nJumlah buku unik dalam data pelatihan: {len(train_books)}")

# TF-IDF Vectorizer pada judul buku dari data pelatihan
tf_eval = TfidfVectorizer()
tfidf_matrix_eval = tf_eval.fit_transform(train_books['Book-Title'].fillna('')) # Isi NaN dengan string kosong

# Cosine Similarity dari matriks TF-IDF pelatihan
cosine_sim_eval = cosine_similarity(tfidf_matrix_eval)

# Petakan ISBN buku pelatihan ke indeks dalam matriks kemiripan
isbn_to_index_eval = {isbn: i for i, isbn in enumerate(train_books['ISBN'])}
index_to_isbn_eval = {i: isbn for i, isbn in enumerate(train_books['ISBN'])}

# --- Fungsi Rekomendasi yang Menggunakan Model Pelatihan ---

def get_content_based_recommendations_eval(user_id, k=10):
    """
    Memberikan rekomendasi Content-Based untuk pengguna berdasarkan buku yang disukai
    di set pelatihan dan kemiripan konten dari model pelatihan.
    """
    # Dapatkan buku yang disukai pengguna di set pelatihan
    user_liked_books_train = train_interaction[(train_interaction['User-ID'] == user_id) & (train_interaction['Book-Rating'] >= 7)] # Asumsi rating >= 7 berarti disukai

    if user_liked_books_train.empty:
        return [] # Tidak ada buku yang disukai di pelatihan, tidak bisa merekomendasikan

    # Ambil ISBN dari buku yang disukai
    liked_book_isbns = user_liked_books_train['ISBN'].tolist()

    # Cari indeks buku yang disukai dalam matriks kemiripan pelatihan
    liked_book_indices = [isbn_to_index_eval[isbn] for isbn in liked_book_isbns if isbn in isbn_to_index_eval]

    if not liked_book_indices:
        return [] # Buku yang disukai tidak ada di set buku pelatihan

    # Hitung skor kemiripan rata-rata dengan semua buku lain
    # Berdasarkan buku-buku yang disukai pengguna
    similarity_scores = np.mean(cosine_sim_eval[liked_book_indices], axis=0)

    # Urutkan buku berdasarkan skor kemiripan (dari tertinggi ke terendah)
    sorted_indices = np.argsort(similarity_scores)[::-1]

    # Dapatkan ISBN buku yang direkomendasikan (selain buku yang sudah disukai pengguna)
    recommended_book_isbns = []
    for index in sorted_indices:
        isbn = index_to_isbn_eval[index]
        # Pastikan buku belum disukai pengguna di set pelatihan
        if isbn not in liked_book_isbns:
            recommended_book_isbns.append(isbn)
            if len(recommended_book_isbns) == k:
                break

    return recommended_book_isbns

# --- Hitung Metrik Evaluasi (Precision@k dan Recall@k) ---

k = 10 # Jumlah rekomendasi (top-k)
total_precision = 0
total_recall = 0
num_users_evaluated = 0

# Evaluasi untuk setiap pengguna dalam set pengujian yang memiliki buku disukai di set pelatihan
users_in_test = test_interaction['User-ID'].unique()

for user_id in users_in_test:
    # Dapatkan buku yang sebenarnya disukai pengguna di set pengujian
    actual_liked_books_test = test_interaction[(test_interaction['User-ID'] == user_id) & (test_interaction['Book-Rating'] >= 7)] # Asumsi rating >= 7 berarti disukai
    actual_liked_book_isbns = actual_liked_books_test['ISBN'].tolist()

    if not actual_liked_book_isbns:
        continue # Pengguna tidak menyukai buku apapun di set pengujian, lewati

    # Dapatkan rekomendasi Content-Based untuk pengguna ini
    recommended_book_isbns = get_content_based_recommendations_eval(user_id, k=k)

    if not recommended_book_isbns:
        continue # Tidak dapat memberikan rekomendasi untuk pengguna ini, lewati

    # Hitung jumlah item yang relevan (item yang disukai pengguna di set pengujian)
    num_relevant_items = len(actual_liked_book_isbns)

    # Hitung jumlah item yang direkomendasikan yang relevan (intersection)
    relevant_recommended_items = len(set(recommended_book_isbns).intersection(set(actual_liked_book_isbns)))

    # Hitung Precision@k dan Recall@k untuk pengguna ini
    precision_at_k = relevant_recommended_items / k if k > 0 else 0
    recall_at_k = relevant_recommended_items / num_relevant_items if num_relevant_items > 0 else 0

    total_precision += precision_at_k
    total_recall += recall_at_k
    num_users_evaluated += 1

# Hitung rata-rata Precision@k dan Recall@k di seluruh pengguna
average_precision_at_k = total_precision / num_users_evaluated if num_users_evaluated > 0 else 0
average_recall_at_k = total_recall / num_users_evaluated if num_users_evaluated > 0 else 0

print(f"\n--- Metrik Evaluasi Content-Based Filtering (k={k}) ---")
print(f"Jumlah pengguna yang dievaluasi: {num_users_evaluated}")
print(f"Average Precision@{k}: {average_precision_at_k:.4f}")
print(f"Average Recall@{k}: {average_recall_at_k:.4f}")

"""Model Content-Based Filtering mencapai Average Precision@10 sebesar 0.0074 dan Average Recall@10 sebesar 0.0556 pada set pengujian. Ini menunjukkan bahwa rata-rata, 0.74% dari 10 rekomendasi teratas relevan bagi pengguna, dan model berhasil menangkap sekitar 5.56% dari total buku yang disukai pengguna dalam 10 rekomendasi teratas.

### Insight

*   Model Content Based Filtering dibangun menggunakan Book-Title sebagai fitur.
*   TfidfVectorizer digunakan untuk mengekstrak fitur penting dari judul buku dan mengubahnya menjadi representasi numerik dalam bentuk matriks TF-IDF.
*   Cosine Similarity dihitung berdasarkan matriks TF-IDF untuk mengukur kemiripan antar buku berdasarkan judulnya.
*   Fungsi book_recommendations dibuat untuk memberikan rekomendasi buku berdasarkan nama penulis, dengan mencari buku-buku yang memiliki kemiripan judul tertinggi.

## Evaluasi

### Model Development dengan Collaborative Filtering

**Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

"""### Data Understanding"""

data = ratings
data.head()

"""### Data Preparation

**Mengubah `User-ID` menjadi list**
"""

users_id = data['User-ID'].unique().tolist()
print('list User-ID: ', users_id)

"""**Melakukan encoding `User-ID`**"""

users_encoded = {x: i for i, x in enumerate(users_id)}
print('Encoding User-ID :',users_encoded)

"""**Melakukan encoding angka ke `User-ID`**"""

users_encoded_to_user = {i: x for i, x in enumerate(users_id)}
print('Encoding angka ke User-ID :', users_encoded_to_user)

books_id = data['ISBN'].unique().tolist()

books_to_book_encoded = {x: i for i, x in enumerate(books_id)}

books_encoded_to_book = {i: x for i, x in enumerate(books_id)}

# Menggunakan .loc untuk menghindari SettingWithCopyWarning
data.loc[:, 'user'] = data['User-ID'].map(users_encoded)
data.loc[:, 'book'] = data['ISBN'].map(books_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(users_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_book = len(books_encoded_to_book)
print(num_book)

# Mengubah rating menjadi nilai float
data.loc[:, 'Book-Rating'] = data['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data['Book-Rating'])

# Nilai maksimal rating
max_rating = max(data['Book-Rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""**Membagi data untuk Training dan Validasi**"""

data = data.sample(frac=1, random_state=42)

data.head()

X = data[['user', 'book']].values

y = data['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * data.shape[0])
X_train, X_val, y_train, y_val = (
    X[:train_indices],
    X[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(X, y)

"""**Proses Training**"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.book_embedding = layers.Embedding(
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    X = dot_user_book + user_bias + book_bias # Variable X is defined here

    return tf.nn.sigmoid(X) # Corrected: using X instead of x

model = RecommenderNet(num_users, num_book, 50)
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (X_val, y_val)
)

df_books = new_book
df = pd.read_csv('/content/book_recommendation/Ratings.csv')
data = df[1:7001]

user_id = data['User-ID'].sample(100).iloc[30]

book_visited_by_user = data[data['User-ID'] == user_id]

book_not_visited = df_books[~df_books['id'].isin(book_visited_by_user.ISBN.values)]['id']
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(books_to_book_encoded.keys()))
)

book_not_visited = [[books_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = users_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

rating = model.predict(user_book_array).flatten()

top_rating_indices = rating.argsort()[-10:][::-1]
recommended_book_ids = [
    books_encoded_to_book.get(book_not_visited[x][0]) for x in top_rating_indices
]

top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

df_books_rows = df_books[df_books['id'].isin(top_book_user)]
for row in df_books_rows.itertuples():
    print(row.book_author, ':', row.book_title)

print('---' * 11)
print('10 Rekomendasi Teratas berdasarkan Book Author')
print('---' * 11)

recommended_book = df_books[df_books['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.book_author, ':', row.book_title)

"""### Insight

*   Model Collaborative Filtering dibangun menggunakan dataset Ratings.
User-ID dan ISBN di-encode menjadi representasi numerik untuk dapat diproses oleh model machine learning.
*   Rentang rating dinormalisasi antara 0 dan 1.
Data dibagi menjadi set pelatihan dan validasi (80:20).
*   Model RecommenderNet berbasis embedding dikembangkan menggunakan TensorFlow/Keras.
*   Model ini mempelajari representasi (embedding) untuk setiap pengguna dan buku, serta bias masing-masing. Rekomendasi dihasilkan dari perkalian dot antara embedding pengguna dan buku, ditambah bias.
*   Model dilatih menggunakan Binary Crossentropy sebagai fungsi loss dan Adam optimizer.

**Evaluasi**

**1. Metrik Root Mean Squared Error (RMSE)**
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""**2. Mean Squared Error (MSE)**"""

from sklearn.metrics import mean_squared_error

print("MSE dari pada data train = ", mean_squared_error(y_true=y_train, y_pred=model.predict(X_train)))
print("MSE dari pada data validation = ", mean_squared_error(y_true=y_val, y_pred=model.predict(X_val)))

"""### Insight

*   Kinerja model Collaborative Filtering dievaluasi menggunakan metrik Root Mean Squared Error (RMSE) dan Mean Squared Error (MSE).
*   Plot dari RMSE pada set pelatihan dan validasi selama pelatihan ditampilkan untuk memantau konvergensi model.
*   Nilai MSE dihitung untuk data pelatihan dan validasi, memberikan indikasi seberapa dekat prediksi model dengan nilai rating sebenarnya.
"""